{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c529bbeb-645f-47c7-a358-e8913689efa4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3643c0f-442f-4165-8be0-f006aaab77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e5fc4-7fd4-48f0-a4f2-9e8c3018a5b9",
   "metadata": {},
   "source": [
    "# Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c07411-cb45-49a1-acb8-2a2739d60f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"api_key\")\n",
    "openai.api_base = os.getenv(\"api_base\")\n",
    "openai.api_type = os.getenv(\"api_type\")\n",
    "openai.api_version = os.getenv(\"api_version\")\n",
    "deployment_id = os.getenv(\"deployment_id_gpt_4\")\n",
    "gpt_model = os.getenv(\"deployment_id_gpt_4\")\n",
    "model_engine = os.getenv(\"deployment_id_gpt_4\")\n",
    "embd_model = 'text-embedding-ada-002'\n",
    "\n",
    "os.environ.update({\n",
    "    \"OPENAI_API_TYPE\": os.getenv(\"api_type\"),\n",
    "    \"OPENAI_API_VERSION\": os.getenv(\"api_version\"),\n",
    "    \"OPENAI_API_BASE\": os.getenv(\"api_base\"),\n",
    "    \"OPENAI_API_KEY\": os.getenv(\"api_key\"),\n",
    "    \"BING_API_KEY\": os.getenv(\"BING_SEARCH_V7_SUBSCRIPTION_KEY\")\n",
    "})\n",
    "\n",
    "bing_api_key = os.getenv(\"BING_API_KEY\")\n",
    "\n",
    "G_BING_SEARCH_V7_SUBSCRIPTION_KEY = \"not_set\"\n",
    "G_BING_SEARCH_V7_ENDPOINT = \"not_set\"\n",
    "\n",
    "\n",
    "def set_global_variable(subscription_key, search_url):\n",
    "    # Use the global keyword to modify the global variable\n",
    "    global G_BING_SEARCH_V7_SUBSCRIPTION_KEY \n",
    "    global G_BING_SEARCH_V7_ENDPOINT\n",
    "    G_BING_SEARCH_V7_SUBSCRIPTION_KEY = subscription_key\n",
    "    G_BING_SEARCH_V7_ENDPOINT = search_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351d862-c654-40bb-a328-d96416ab9c7e",
   "metadata": {},
   "source": [
    "# Serch topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d3f9d-43ae-436c-bf09-3695fcc03017",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_categories = [\"Energy Transition\", \n",
    "                   \"Nuclear Energy\", \n",
    "                   \"Oil & Gas\",\n",
    "                   \"Renewable Energy\",\n",
    "                    \"Solar Energy\",\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220423df-a6f6-4a70-bfa8-8a7cf4b75f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracts the text content from the first <h1> tag found in the HTML of the specified URL.\n",
    "def extract_h1_tag_text(url):\n",
    "    # Make a GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = Soup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all <h1> tags in the parsed HTML\n",
    "    h1_tags = soup.find_all('h1')\n",
    "\n",
    "    # Return the text content of the first <h1> tag found, or None if no <h1> tag is present\n",
    "    for tag in h1_tags:\n",
    "        return tag.text\n",
    "\n",
    "## Convert to plain text\n",
    "def html_to_plain_text(html_string):\n",
    "    soup = Soup(html_string, 'html.parser')\n",
    "    plain_text = soup.get_text(separator=' ', strip=True)\n",
    "    return plain_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30935bd1-3417-408b-87ef-9dc0aa7a1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_url(search_term):\n",
    "    subscription_key = G_BING_SEARCH_V7_SUBSCRIPTION_KEY\n",
    "    search_url = G_BING_SEARCH_V7_ENDPOINT\n",
    "\n",
    "    # print(f\"++++++++++++++++++{search_term}\")\n",
    "    \n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "    params = {\"q\": search_term, \"textDecorations\": True, \"textFormat\": \"HTML\", \"count\":20}\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    search_results = response.json()\n",
    "    # print(search_results)\n",
    "\n",
    "    results=search_results['value']\n",
    "\n",
    "    names=[x['name'] for x in results]\n",
    "    urls=[x['url'] for x in results]\n",
    "\n",
    "    final_urls = []\n",
    "    for name, url in zip(names, urls):\n",
    "        name = html_to_plain_text(name)\n",
    "        final_urls.append(\"[\"+name+\"](\"+url+\")\")\n",
    "    # print(f\">>>> {len(final_urls)}\")\n",
    "    return final_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ba3ed-7e0f-4e96-8cd3-a7e59c6e2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_func_for_news_summarization(web_content):\n",
    "    system_content = \"\"\"analyze the given text string passed as user input, you have to find the below information.\n",
    "        information 1. What is the website name from where the input text is read, give the precise answer only.\n",
    "        information 2. summary_para : summary in paragraph format within 100 words in a professional way. NOTE: if the input text talks about 'Page Not Found', return summary as \"not available\".\n",
    "        information 3. summary_bullet : summary in bullet 7 tp 10 bullet points, this should be in str format not in list format in a professional way. NOTE: if the input text talks about 'Page Not Found', return summary as \"not available\"..\n",
    "        information 4. category : find out that the input text belongs to which all of the following Five categories, it can be more than one categories as well, if the text does not belongs to any of the five catagories, catagorize it as others. Append the categories as comma seprated string, the final output should be a string not a list. \n",
    "            1. Regulatory and Compliance \n",
    "                definition of this category: news about government policies, regulations, and legal developments affecting the energy sector.\n",
    "            2. Sustainability \n",
    "                definition of this category: News related to the environmental impact of energy production and efforts toward sustainability \n",
    "            3. Corporate News\n",
    "                definition of this category: Updates and developments from energy companies, mergers and acquisitions \n",
    "            4. Financial\n",
    "                definition of this category: Financial report of organizations. \n",
    "            5. Geopolitics\n",
    "                definition of this category: International energy-related news, collaborations, and geopolitical factors influencing the energy sector.\n",
    "         information 5. sentiment score: Analyze the input text and find the sentiment score range within -1 to +1 where -1.0 is negative sentiment and +1.0 positive sentiment, make sure the score is generated precisely within -1.0 to +1.0, nothing else.\n",
    "         information 6. Justify the sentiment score calculated in 50 words, the output should talk about article instead of text and should be in string format.\n",
    "         information 7. Create a headline within 10 words based on the text.\n",
    "         information 8. Analyse the input article and determine that this article belongs to which country, make sure to return one country only. If you are not sure about the answer return \"Others\".\n",
    "            \n",
    "        Instructions:\n",
    "        - Find all four information as per the instruction.\n",
    "        - Output the results in JSON format as shown in the example.\n",
    "        - If any of the information is not found return it as 'not available'.\n",
    "        - The input text can belong to multiple categories, so it should be in list of categories mentioned above.\n",
    "        - Present the summaries as strings, not in a list.\n",
    "        - Make sure that the final output is in JSON FORMAT ONLY.\n",
    "        - If the input text is about page error 404 or page error 403, return summary_para and summary_bullet \"not available\"\n",
    "        - sentiment_score should be a number between -1.0 and +1.0 only, if you are not able to determine the score, return it as 0.0\n",
    "        - NO EXTRA DATA other than JSON in the output.\n",
    "        - for specific region If the country name is provided in an abbreviated or alternative form, please convert it to the standard format. For example, replace 'UK' with 'United Kingdom' and 'EU' or 'Europe' with 'European Union' and 'US' or 'USA' with 'United States\n",
    "        - Take your time to generate the outputs.\n",
    "\n",
    "        Example JSON format:\n",
    "        {\n",
    "            \"Source\" : <website name>,\n",
    "            \"summary_para\" : \"\",\n",
    "            \"summary_bullet\" : \"\",\n",
    "            \"category\" : [],\n",
    "            \"sentiment_score\" : <sentiment_score>,\n",
    "            \"sentiment_justification\" : \"\",\n",
    "            \"gen_headline\" : <headline>,\n",
    "            \"specific_region\" : \"\"\n",
    "        }\n",
    "        \"\"\"    \n",
    "    conversation = [{\"role\": \"system\", \"content\":system_content}]\n",
    "    user_input = {\"role\":\"user\", \"content\":web_content}\n",
    "\n",
    "    conversation.append(user_input)\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(engine=model_engine, messages=conversation, temperature=0.3)\n",
    "        details = response['choices'][0]['message']['content']\n",
    "        details = details.strip()\n",
    "    except:\n",
    "        details = \"\"\"{\"Source\" : \"not available\",\n",
    "                       \"summary_para\": \"not available\", \n",
    "                       \"summary_bullet\": \"not available\",  \n",
    "                       \"category\": \"not available\", \n",
    "                       \"sentiment_score\": \"not available\",\n",
    "                       \"sentiment_justification\" : \"not available\",\n",
    "                        \"gen_headline\" : \"not available\"}\"\"\"\n",
    "    \n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded54a1-f90c-470c-9cce-559ae70602ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to perform Bing News search for a perticular topic\n",
    "def bing_news_articles(search_term):\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    subscription_key = G_BING_SEARCH_V7_SUBSCRIPTION_KEY\n",
    "    search_url = G_BING_SEARCH_V7_ENDPOINT\n",
    "    \n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "    params = {\"q\": search_term, \"textDecorations\": True, \"textFormat\": \"HTML\", \"freshness\": \"Week\", \"originalImg\":True, \"count\":20}\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    search_results = response.json()\n",
    "    # print(search_results)\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9e83c-c4c2-435a-a369-1289f17f4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create a folder\n",
    "def create_folder(folder_name):\n",
    "    # Get the current date in the format YYYY-MM-DD\n",
    "    # current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    current_date = \"current\"\n",
    "    current_date = current_date+\"_bing_websearch\"\n",
    "\n",
    "    # Create the parent folder with the current date\n",
    "    parent_folder_path = os.path.join(os.getcwd(), current_date)\n",
    "    parent_folder_path = parent_folder_path\n",
    "    os.makedirs(parent_folder_path, exist_ok=True)\n",
    "\n",
    "    # Create the nested folder with the provided name\n",
    "    nested_folder_path = os.path.join(parent_folder_path, folder_name)\n",
    "    \n",
    "    # Check if the folder already exists\n",
    "    if os.path.exists(nested_folder_path):\n",
    "        return \"already_exists\"\n",
    "    \n",
    "    # Create the nested folder\n",
    "    os.makedirs(nested_folder_path)\n",
    "\n",
    "    return current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30d3aa-1c84-4922-865c-0cc249e1facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to perform web scrapping with timeout\n",
    "def perform_web_scrapping(url):\n",
    "    page_content = \"\"\n",
    "    try:\n",
    "        loader = RecursiveUrlLoader(\n",
    "            url=url, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text, timeout=10\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        page_content = docs[0].page_content\n",
    "    except:\n",
    "        page_content = \"Timed out !!!!\"\n",
    "    return page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff02580-4490-4913-9414-3f01e60afb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorizes news sentiments into positive, negative, or neutral based on the provided sentiment scores.\n",
    "def categorize_news(sentiments):\n",
    "    label_news = lambda x: f\"{x} (Positive news)\" if float(x) >= 0.3 else (f\"{x} (Negative news)\" if float(x) <= -0.3 else f\"{x} (Neutral news)\") \n",
    "    categorized_list = list(map(label_news, sentiments))\n",
    "    return categorized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d350ed6-0d45-47bf-a08c-a7c6ab9dd601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv_files(input_folder, output_file):\n",
    "    # Get a list of all subdirectories in the input folder\n",
    "    subdirectories = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "\n",
    "    # Initialize an empty DataFrame to store the merged data\n",
    "    merged_data = pd.DataFrame()\n",
    "\n",
    "    # Loop through each subdirectory\n",
    "    for subdir in subdirectories:\n",
    "        # Construct the path to the news_df.csv file in the current subdirectory\n",
    "        csv_file_path = os.path.join(input_folder, subdir, 'news_df.csv')\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(csv_file_path):\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "\n",
    "            # Merge the data into the main DataFrame\n",
    "            merged_data = pd.concat([merged_data, df], ignore_index=True)\n",
    "            \n",
    "    ############### adding similar URL ######################\n",
    "    \n",
    "    df=merged_data.copy()\n",
    "    \n",
    "    # df['date'] = df['date'].apply(lambda date: date.split('T')[0])\n",
    "    df['summary_bullet'] = df['summary_bullet'].apply(lambda x: x.replace('â€¢',''))\n",
    "\n",
    "    # Write the merged data to a new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f'Merged data saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac125ea0-3a2d-4b90-86ae-fbc9d9a7b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters as it is noticed while performing bing search a special char gives bad results.\n",
    "def remove_special_characters(input_string):\n",
    "    # Define a regular expression pattern to match all non-alphanumeric characters except ',' and '.'\n",
    "    pattern = re.compile('[^a-zA-Z0-9,.\\s]')\n",
    "    # Use the pattern to replace matched characters with an empty string\n",
    "    result_string = re.sub(pattern, '', input_string)\n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2feea7c-c96e-4bb3-b03e-239645ce3375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_category_news(current_category):\n",
    "    # Lists to store information for each news article\n",
    "    Category = [current_category] * 10\n",
    "    description = []\n",
    "    provider = []\n",
    "    url = []\n",
    "    source = []\n",
    "    tag = []\n",
    "    formatted_time = []\n",
    "    summary_para = []\n",
    "    summary_bullet = []\n",
    "    similar_url = []\n",
    "    image_url = []\n",
    "    senti_score = []\n",
    "    senti_justification = []\n",
    "    headline_txt = []\n",
    "    generated_headline = []\n",
    "    curr_region = []\n",
    "\n",
    "    news_article_count = 0\n",
    "\n",
    "    fall_back_images = [\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSxSRYFpmfDLL131rXdBuNFwuZYXqXh7m7pf-PGDDAlHVCK-MtupIHcv9StNPnG5ukIaqE&usqp=CAU\",\n",
    "                        \"https://www.financialexpress.com/wp-content/uploads/2023/12/solar2-1.jpg?w=1024\",\n",
    "                        \"https://cepa.org/wp-content/uploads/2022/07/2022-03-24T000000Z_753859509_MT1NURPHO000K7054A_RTRMADP_3_GERMANY-ENERGY.jpg\",\n",
    "                        \"https://electricalreview.co.uk/wp-content/uploads/2023/12/energy-transition.jpg\"]\n",
    "    \n",
    "    # create folder of current date name\n",
    "    folder_name = create_folder(current_category)\n",
    "\n",
    "    if folder_name == \"already_exists\":\n",
    "        print(\"Today's News is already extracted\")\n",
    "        return\n",
    "    \n",
    "    # Search news now\n",
    "    search_results = bing_news_articles(current_category)\n",
    "  \n",
    "    for article in search_results[\"value\"]:\n",
    "        news_url = article[\"url\"]\n",
    "        print(\"\\n\\n------------------------------------ >>>>>>\")\n",
    "        print(f\"{current_category}: {news_url}\")\n",
    "\n",
    "        news_txt = perform_web_scrapping(article[\"url\"])\n",
    "        \n",
    "        news_txt_len = len(news_txt)\n",
    "        print(f\"-------- news txt len = {news_txt_len}--------------\")\n",
    "        if news_txt_len < 350:\n",
    "            print(\"Since not enough text available, skipping this news article.\")\n",
    "            continue\n",
    "\n",
    "        print(\"%%%%%%%%%%%%%\")\n",
    "        print(article)\n",
    "        print(\"%%%%%%%%%%%%%\")\n",
    "    \n",
    "        description.append(article[\"description\"])\n",
    "\n",
    "        # datePublished.append(article[\"datePublished\"])\n",
    "        publish_date = pd.to_datetime(article[\"datePublished\"])\n",
    "        formatted_time_val = publish_date.strftime('%d %B %Y') \n",
    "        formatted_time.append(formatted_time_val)\n",
    "\n",
    "        image_tmp = article.get('image', \"not_available\")\n",
    "        # print(image_tmp)\n",
    "        if image_tmp == \"not_available\":\n",
    "            random_number = random.randint(0, 3)\n",
    "            image_url.append(fall_back_images[random_number])\n",
    "        else:\n",
    "            image_url.append(image_tmp[\"contentUrl\"])\n",
    "            # image_url.append(image_tmp[\"thumbnail\"][\"contentUrl\"])\n",
    "        provider.append(article[\"provider\"][0][\"name\"])\n",
    "        url.append(article[\"url\"])\n",
    "\n",
    "        # Call the gpt end point to summarize the \n",
    "        news_summary = gpt_func_for_news_summarization(news_txt)\n",
    "        \n",
    "        print(news_summary)\n",
    "        news_summary = json.loads(news_summary)\n",
    "\n",
    "        if news_summary[\"summary_para\"] == \"not available\":\n",
    "            print(f\"<<<<<< ------------------------------------{news_article_count} \")\n",
    "            continue\n",
    "\n",
    "        headline = extract_h1_tag_text(article[\"url\"])\n",
    "        if headline is None:\n",
    "            headline = \" \"\n",
    "        headline = headline.strip()\n",
    "\n",
    "        if len(headline) < 1 or \"error\" in headline.lower():\n",
    "            headline = news_summary[\"gen_headline\"]\n",
    "\n",
    "        headline_txt.append(headline)\n",
    "        \n",
    "        similer_url_query_term_tmp = headline + ', ' + formatted_time_val\n",
    "        similer_url_query_term = remove_special_characters(similer_url_query_term_tmp)\n",
    "        sim_urls = get_similar_url(similer_url_query_term)\n",
    "        similar_urls = \"\\n\".join(list(sim_urls[1:4]))\n",
    "        similar_url.append(similar_urls)  \n",
    "    \n",
    "        summary_para.append(news_summary[\"summary_para\"])\n",
    "        summary_bullet.append(news_summary[\"summary_bullet\"])\n",
    "        tag.append(news_summary[\"category\"])\n",
    "        score = categorize_news([news_summary[\"sentiment_score\"]])\n",
    "        senti_score.append(score[0])\n",
    "        senti_justification.append(news_summary[\"sentiment_justification\"])\n",
    "        source.append(news_summary[\"Source\"])\n",
    "        generated_headline.append(news_summary[\"gen_headline\"])\n",
    "        curr_region.append(news_summary[\"specific_region\"])\n",
    "\n",
    "        # just to count how many news articles are successfully processed.\n",
    "        news_article_count += 1\n",
    "        print(f\"------------------------------------{news_article_count} >>>>>>>>\")\n",
    "        if news_article_count >= 3:\n",
    "            break\n",
    "    \n",
    "    # Create a DataFrame of this data\n",
    "    news_df = pd.DataFrame(list(zip(Category, description, provider, curr_region, url, tag, formatted_time, headline_txt, generated_headline, summary_para, summary_bullet, image_url, senti_score, senti_justification, similar_url)),\n",
    "        columns=[\"Category\", \"News Article\", \"News channel Source\", \"region\", \"Web link\", \"tag\", \"formatted_timestamp\", \"headline_text\", \"generated_headline\", \"summary_para\", \"summary_bullet\", \"image_url\", \"sentiment_score\", \"sentiment_justification\", \"similar_url\"])\n",
    "    \n",
    "    # Write this to folder\n",
    "    print(news_df)\n",
    "    news_df.to_csv(\".//\" + folder_name + \"//\" + current_category + \"//news_df.csv\")\n",
    "\n",
    "    return folder_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d40ba-7566-4973-b06e-df1320f7aa73",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6d9fb-8894-41b1-87a0-fa43c7dacb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    subscription_key = os.getenv(\"BING_SEARCH_V7_SUBSCRIPTION_KEY\")\n",
    "    search_url = os.getenv(\"BING_SEARCH_V7_ENDPOINT_NEWS\")\n",
    "\n",
    "    # set global variable\n",
    "    set_global_variable(subscription_key, search_url)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for cat in news_categories:\n",
    "        print(\">>> searching for :\", cat)\n",
    "        folder_name = get_specific_category_news(cat)\n",
    "        # Example usage:\n",
    "        input_folder = folder_name\n",
    "        output_file = './/'+folder_name+'//merged_news.csv'\n",
    "        \n",
    "        merge_csv_files(input_folder, output_file)\n",
    "        \n",
    "        # Record the end time\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "    print(f\"Execution time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc6fa8-31dd-406f-8b3a-5beef08e8090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11_news",
   "language": "python",
   "name": "py11_news"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
